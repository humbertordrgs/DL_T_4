{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Respuestas_Tarea_4_ConvNet_CC6204_2020",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhjpqvcdo5o"
      },
      "source": [
        "# Tarea 4: Redes Convolucionales <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
        "\n",
        "## Nombre: \n",
        "Fecha de entrega: 11 de diciembre de 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BkmYga3UN_"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
        "if not os.path.exists('utils.py'):\n",
        "  !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea4/utils.py\n",
        "\n",
        "from utils import ImageCaptionDataset, train_for_classification, train_for_retrieval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtwf7_btPh7n"
      },
      "source": [
        "# Aqui instalamos la libreria de correccion del curso\n",
        "!pip install -U \"git+https://github.com/dccuchile/CC6204.git@master#egg=cc6204&subdirectory=autocorrect\"\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# importamos las herramientas del curso\n",
        "from cc6204 import AutoCorrect, FailedTest\n",
        "\n",
        "# En caso que se les indique, cambia el host y port que posteamos en u-cursos\n",
        "corrector = AutoCorrect(host=\"cc6204.dcc.uchile.cl\", port=443)\n",
        "\n",
        "# En caso que se les indique, cambia el token que te daremos en u-cursos\n",
        "token = \"]ye/Ox;nsz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnux9hNPSVYv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(loss, score1, score1_title='Accuracy', score2=None, score2_title=None):\n",
        "  f1 = plt.figure(1)\n",
        "  ax1 = f1.add_subplot(111)\n",
        "  ax1.set_title(\"Loss\")    \n",
        "  ax1.set_xlabel('epochs')\n",
        "  ax1.set_ylabel('loss')\n",
        "  ax1.plot(loss, c='r')\n",
        "  ax1.legend(['train-loss'])\n",
        "  f1.show()\n",
        "\n",
        "  f2 = plt.figure(2)\n",
        "  ax2 = f2.add_subplot(111)\n",
        "  ax2.set_title(score1_title)    \n",
        "  ax2.set_xlabel('epochs')\n",
        "  ax2.set_ylabel(score1_title.lower())\n",
        "  ax2.plot(score1[0], c='b')\n",
        "  ax2.plot(score1[1], c='g')\n",
        "  ax2.legend([f'train-{score1_title.lower()}', f'val-{score1_title.lower()}'])\n",
        "  f2.show()\n",
        "\n",
        "  if score2:\n",
        "    f3= plt.figure(3)\n",
        "    ax3 = f3.add_subplot(111)\n",
        "    ax3.set_title(score2_title)    \n",
        "    ax3.set_xlabel('epochs')\n",
        "    ax3.set_ylabel(score2_title.lower())\n",
        "    ax3.plot(score2[0], c='b')\n",
        "    ax3.plot(score2[1], c='g')\n",
        "    ax3.legend([f'train-{score2_title.lower()}', f'val-{score2_title.lower()}'])\n",
        "    f3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mw8PKh-uF7P"
      },
      "source": [
        "# Parte 1: Arquitectura Convolucional GoogLeNet (y otras) para CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qzrGbKRRxh"
      },
      "source": [
        "## 1a) Inception Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVeNZDV8RlDr"
      },
      "source": [
        "class InceptionModule(nn.Module):\n",
        "  def __init__(self, \n",
        "               in_channels, \n",
        "               ch_3x3_reduce=96, \n",
        "               ch_5x5_reduce=16,\n",
        "               ch_3x3=128,\n",
        "               ch_5x5=32,\n",
        "               ch_pool_proj=32,\n",
        "               ch_1x1=64\n",
        "    ):\n",
        "    super(InceptionModule, self).__init__()\n",
        "    # Acá inicializa todos los parámetros\n",
        "    ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Calcula la salida como un tensor con cantidad de canales de\n",
        "    # salida dado por ch_3x3 + ch_5x5 + ch_pool_proj + ch_1x1\n",
        "    ...\n",
        "\n",
        "    return ...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5AxECptzqin"
      },
      "source": [
        "# Tests del API del curso para el InceptionModule\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementación\n",
        "x, in_chs, ch_1x1, ch_3x3_red, ch_3x3, ch_5x5_red, ch_5x5, ch_pool_proj = corrector.get_test_data(homework=4, question=\"1a\", test=1, token=token)\n",
        "\n",
        "# Corramos tu implementación de InseptionModule para ver como se comporta\n",
        "with torch.no_grad():\n",
        "  model = InceptionModule(in_chs, ch_1x1, ch_3x3_red, ch_3x3, ch_5x5_red, ch_5x5, ch_pool_proj)\n",
        "  s = timer()\n",
        "  result = model(torch.tensor(x))\n",
        "  t = timer()-s\n",
        "\n",
        "# Veamos si todo fue OK :)\n",
        "corrector.submit(homework=4, question=\"1a\", test=1, token=token, answer=list(result.size()), time=t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJG69heDRag8"
      },
      "source": [
        "## 1b) GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd3iSAd7oXKN"
      },
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "  def __init__(self, n_classes, use_aux_logits=True):\n",
        "    super(GoogLeNet, self).__init__()\n",
        "\n",
        "    # Define las capas de convolución y pooling de GoogLeNet\n",
        "    ...\n",
        "\n",
        "    # Decide si usar la clasificación auxiliar\n",
        "    self.use_aux_logits = use_aux_logits\n",
        "    if self.use_aux_logits:\n",
        "      # Acá lo que necesites inicializar en este caso\n",
        "      ...\n",
        "\n",
        "    # Capa de salida (antes de la función de salida)\n",
        "    self.fc_out = nn.Linear(..., n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Crea una lista para los logits auxiliares si fuera necesario\n",
        "    if self.use_aux_logits and self.training:\n",
        "      aux_logits = []\n",
        "    else:\n",
        "      aux_logits = None\n",
        "\n",
        "    # Computa las representaciones internas de la red\n",
        "    ...\n",
        "\n",
        "    # Si se usa la clasificación auxiliar, computa logits auxiliares\n",
        "    if self.use_aux_logits and self.training:\n",
        "      aux_logit_1 = ...\n",
        "      # Agrégalo a la lista de logits auxiliares\n",
        "      aux_logits.append(aux_logit_1)\n",
        "\n",
        "    # Continúa computando las representaciones internas de la red\n",
        "    ...\n",
        "\n",
        "    # Si se usa la clasificación auxiliar, computa logits auxiliares\n",
        "    aux_logit_2 = None\n",
        "    if self.use_aux_logits and self.training:\n",
        "      aux_logit_2 = ...\n",
        "      # Agrégalo a la lista de logits auxiliares\n",
        "      aux_logits.append(aux_logit_2)\n",
        "\n",
        "    # Continúa computando las representaciones internas de la red\n",
        "    ...\n",
        "\n",
        "    # N x out_size\n",
        "    logits = self.fc_out(...)\n",
        "\n",
        "    # En hidden debes devolver alguna de las capas oculta de la red\n",
        "    return {'hidden': ..., 'logits': logits, 'aux_logits': aux_logits}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQs327QW0Fnd"
      },
      "source": [
        "# Tests del API del curso para el InceptionModule\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementación\n",
        "x, n_classes, use_aux_logits = corrector.get_test_data(homework=4, question=\"1b\", test=1, token=token)\n",
        "\n",
        "# Corramos tu implementación de InseptionModule para ver como se comporta\n",
        "with torch.no_grad():\n",
        "  model = GoogLeNet(n_classes=n_classes, use_aux_logits=use_aux_logits)\n",
        "  s = timer()\n",
        "  result = model(torch.tensor(x))\n",
        "  t = timer()-s\n",
        "\n",
        "# Veamos si todo fue OK :)\n",
        "sizes = [result['hidden'].shape[0]] + list(result['logits'].size()) + [d for a in result['aux_logits'] for d in a.size()]\n",
        "corrector.submit(homework=4, question=\"1b\", test=1, token=token, answer=sizes, time=t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTQpC0eCKp3"
      },
      "source": [
        "## 1c) Arquitectura Convolucional: _____\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMLkLKGFoTKa"
      },
      "source": [
        "# Acá el código para tu primera arquitectura\n",
        "\n",
        "class ...(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(..., self).__init__()\n",
        "\n",
        "    # Define las capas de convolución y pooling de tu arquitectura\n",
        "    ...\n",
        "\n",
        "    # Capa de salida (antes de la función de salida)\n",
        "    self.fc_out = nn.Linear(..., n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Computa las representaciones internas de la red\n",
        "    ...\n",
        "\n",
        "    # N x out_size\n",
        "    logits = self.fc_out(...)\n",
        "\n",
        "    # En hidden debes devolver alguna de las capas oculta de la red\n",
        "    return {'hidden': ..., 'logits': logits}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE6uDwmJ94-W"
      },
      "source": [
        "## 1d) Clasificación de Imágenes en CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1InBxxn28TJ4"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar después. Después de ejecutar las celdas tendrás los datos \n",
        "# trainset, trainloader y similar para test.\n",
        "##############################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecHwyZa6oxMc"
      },
      "source": [
        "# Definamos algunos hiper-parámetros\n",
        "BATCH_SIZE = ...\n",
        "LR = ...\n",
        "EPOCHS = ...\n",
        "REPORTS_EVERY = 1\n",
        "\n",
        "net = ... # tu modelo de CNN (para clasificar en 10 clases)\n",
        "optimizer = ... # optimizador, e.g., optim.SGD, optim.Adam, ...\n",
        "criterion = nn.CrossEntropyLoss() # función de pérdida\n",
        "scheduler = ... # (opcional) optim.lr_scheduler proporciona varios métodos para ajustar el lr según el número de épocas\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=4*BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, acc = train_for_classification(net, train_loader, \n",
        "                                           test_loader, optimizer, \n",
        "                                           criterion, lr_scheduler=scheduler, \n",
        "                                           epochs=EPOCHS, reports_every=REPORTS_EVERY)\n",
        "\n",
        "plot_results(train_loss, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoEdEQnedJMY"
      },
      "source": [
        "# Test\n",
        "x, y = list(test_loader)[0]\n",
        "net.cpu()\n",
        "net.eval()\n",
        "y_pred = net(x)['logits'].max(dim=1)[1]\n",
        "\n",
        "# Veamos como se comporta el modelo\n",
        "print(\"Correct Test!\" if (y==y_pred).sum()/len(x) >= .75 else \"Failed Test! [acc]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmALm7EtpFow"
      },
      "source": [
        "## 1e) Opcional: CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prIQA-PjpqV7"
      },
      "source": [
        "##############################################################################\n",
        "# Toda esta parte es similar a la anterior pero para CIFAR100.\n",
        "##############################################################################\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data/cifar100', train=False,\n",
        "                                        download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDSB4v2x8T3k"
      },
      "source": [
        "# Definamos algunos hiper-parámetros\n",
        "BATCH_SIZE = ...\n",
        "LR = ...\n",
        "EPOCHS = ...\n",
        "REPORTS_EVERY = 1\n",
        "\n",
        "net = ... # tu modelo de CNN (para clasificar en 100 clases)\n",
        "optimizer = ... # optimizador, e.g., optim.SGD, optim.Adam, ...\n",
        "criterion = nn.CrossEntropyLoss() # función de pérdida\n",
        "scheduler = ... # (opcional) optim.lr_scheduler proporciona varios métodos para ajustar el lr según el número de épocas\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=4*BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, acc = train_for_classification(net, train_loader, \n",
        "                                           test_loader, optimizer, \n",
        "                                           criterion, lr_scheduler=scheduler, \n",
        "                                           epochs=EPOCHS, reports_every=REPORTS_EVERY)\n",
        "\n",
        "plot_results(train_loss, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRNeU6di3NzC"
      },
      "source": [
        "# Parte 2: Subtitulado de Imágenes mediante Recuperación de Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twAZ0BtioCT0"
      },
      "source": [
        "## 2a) Codificación de Imágenes y Textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9zd0b1MyAG8"
      },
      "source": [
        "class ImageEncoding(nn.Module):\n",
        "  def __init__(self, cnn_model, cnn_out_size, out_size=128):\n",
        "    super(ImageEncoding, self).__init__()\n",
        "    self.cnn_model = cnn_model\n",
        "\n",
        "    # Defina las capas de su MLP\n",
        "    # Hints: no usar más de 3 capas\n",
        "    #        incorpora alguna técnica de regularización que ya conoces\n",
        "    ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn_model(x)['hidden']\n",
        "\n",
        "    # Compute las capas de su MLP\n",
        "    ...\n",
        "\n",
        "    # En fc_out debe almacenar el encoding en R^d\n",
        "    return {'logits': ...}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ7NXhVY0xFr"
      },
      "source": [
        "class TextEncoding(nn.Module):\n",
        "  def __init__(self, text_embedding_size=4096, out_size=128):\n",
        "    super(TextEncoding, self).__init__()\n",
        "\n",
        "    # Defina las capas de su MLP\n",
        "    # Hints: no usar más de 3 capas\n",
        "    #        incorpora alguna técnica de regularización que ya conoces\n",
        "    ...\n",
        "\n",
        "    self.use_last_bn = use_last_bn\n",
        "    if use_last_bn:\n",
        "      self.bn = nn.BatchNorm1d(out_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Compute las capas de su MLP\n",
        "    ...\n",
        "\n",
        "    # En logits debe almacenar el encoding en R^d\n",
        "    return {'logits': ...}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08sa7MG4c-GD"
      },
      "source": [
        "# Test\n",
        "OUT_SIZE = 200\n",
        "\n",
        "cnn_net = GoogLeNet()\n",
        "i_enc = ImageEncoding(cnn_model=cnn_net, cnn_out_size=1024, out_size=OUT_SIZE)\n",
        "t_enc = TextEncoding(text_embedding_size=4096, out_size=OUT_SIZE)\n",
        "i_enc.eval()\n",
        "t_enc.eval()\n",
        "\n",
        "# Veamos como se comportan tus encoders\n",
        "print(\"Correct Test!\" if (i_enc(torch.randn(9,3,32,32))['logits'].size()==t_enc(torch.randn(9,4096))['logits'].size()) else \"Failed Test [size]\")\n",
        "print(\"Correct Test!\" if (i_enc(torch.randn(9,3,32,32))['logits'].size(-1)==OUT_SIZE) else \"Failed Test [size]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR8AqpDi3ZJL"
      },
      "source": [
        "## 2b) Buenas codificaciones y la *Triplet Loss*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv0oqLTwv68U"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "  def __init__(self, margin=.2, negative='max'):\n",
        "    super(TripletLoss, self).__init__()\n",
        "    self.margin = margin\n",
        "    self.negative = negative\n",
        "\n",
        "  def forward(self, anchor, positive):\n",
        "    # Posiblemente lo más simple es partir calculando la distancia Euclideana\n",
        "    # entre las imagenes ancla y todos los pares (B x B) de representaciones\n",
        "    # de textos (hint: usa torch.cdist)\n",
        "    dists = ...\n",
        "\n",
        "    # Obtener distancias \"positivas\" de la diagonal\n",
        "    p_dists = ...\n",
        "    p_dist = p_dist.unsqueeze(1).expand_as(dists)\n",
        "\n",
        "    # Ahora genera un tensor con todos los costos\n",
        "    # siguiendo la formula vista en la orientación de la tarea\n",
        "    cost = (...).clamp(min=0).fill_diagonal_(0)\n",
        "\n",
        "    # Ahora genera un tensor con todos los costos que se deben agregar\n",
        "    # dependiendo de la forma de encontrar los negativos\n",
        "    if self.negative == 'max':\n",
        "      cost = ...\n",
        "    elif self.negative == 'random':\n",
        "      # creamos tansor con los pesos para hacer la selección elatoria\n",
        "      # (una matriz con 1 en todas las posiciones, excepto la diagonal)\n",
        "      weight = ...\n",
        "      # seleccionamos aleatoriamente un costo negativo por cada anchor\n",
        "      ids = torch.multinomial(..., num_samples=...)\n",
        "      cost = cost.gather(1, ids)\n",
        "    elif not self.negative == 'all':\n",
        "      raise ValueError()\n",
        "    \n",
        "    # Retorna el promedio de los costos de todos los triples considerados\n",
        "    return cost[cost>0].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HKUFhf70nbd"
      },
      "source": [
        "# Tests del API del curso para TripletLoss\n",
        "\n",
        "# Obtengamos algunos parametros para probar tu implementación\n",
        "for test in [1,2]:\n",
        "  a, p, m, n  = corrector.get_test_data(homework=4, question=\"2b\", test=test, token=token)\n",
        "\n",
        "  criterion = TripletLoss(margin=m, negative=n)\n",
        "  result = criterion(torch.tensor(a), torch.tensor(p)).item()\n",
        "\n",
        "  # Veamos si todo fue OK :)\n",
        "  corrector.submit(homework=4, question=\"2b\", test=test, token=token, answer=result, time=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZcwMyTAwz28"
      },
      "source": [
        "## 2c) Probando tu implementación en Flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E06OpFAxfuU"
      },
      "source": [
        "##############################################################################\n",
        "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
        "# listos para usar después. Después de ejecutar las dos celdas siguientes\n",
        "# tendrás los datos en train_flickr_tripletset y similar para val y test\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/flickr8k'\n",
        "if not os.path.exists(f'{folder_path}/images'):\n",
        "  print('\\n*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...')\n",
        "  print('****** Descargando las imágenes...\\n')\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images\n",
        "  print('\\n********* Extrayendo las imágenes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "  !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
        "  print('\\n*** Descargando y anotaciones de la imágenes...\\n')\n",
        "  !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Resize((32, 32)),\n",
        "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "print('Inicializando pytorch Flickr8k dataset')\n",
        "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset',\n",
        "                                                ann_file = f'{folder_path}/annotations/8k-pictures.html',\n",
        "                                                transform=transform)\n",
        "print('Creando train, val y test splits...')\n",
        "\n",
        "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
        "for i, item in enumerate(full_flickr_set):\n",
        "  if i<6000:\n",
        "    train_flickr_set.append(item)\n",
        "  elif i<7000:\n",
        "    val_flickr_set.append(item)\n",
        "  else:\n",
        "    test_flickr_set.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTY5bha_xgCj"
      },
      "source": [
        "##############################################################################\n",
        "# Descarguemos representaciones de los textos de 4096 dimensiones\n",
        "##############################################################################\n",
        "if not os.path.exists(f'{folder_path}/flickr_cap_encodings_4096d.pkl'):\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/flickr_cap_encodings_4096d.pkl -P $folder_path\n",
        "\n",
        "with open(f'{folder_path}/flickr_cap_encodings_4096d.pkl', 'rb') as f:\n",
        "  train_cap_encs, val_cap_encs, test_cap_encs = pickle.load(f)\n",
        "\n",
        "# Creamos un dataset para cada uno de los splits con nuestro ImageCaptionDataset\n",
        "train_flickr_tripletset = ImageCaptionDataset(train_flickr_set, train_cap_encs)\n",
        "val_flickr_tripletset = ImageCaptionDataset(val_flickr_set, val_cap_encs)\n",
        "test_flickr_tripletset = ImageCaptionDataset(test_flickr_set, test_cap_encs)\n",
        "\n",
        "##############################################################################\n",
        "# Acá termina el código para preparar los datos\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THIoPtv-w2QY"
      },
      "source": [
        "##############################################################################\n",
        "# Esta es la parte donde tienes que modificar para poder probar tu \n",
        "# implementación. \n",
        "# En general sólo es necesario que modifiques los lugares con \"...\", pero \n",
        "# eres libre de hacer tus propias implementaciones de todo lo que aparece.\n",
        "##############################################################################\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-4\n",
        "EPOCHS = ...\n",
        "REPORTS_EVERY = 1\n",
        "CNN_OUT_SIZE = ...\n",
        "EMBEDDING_SIZE = 4096\n",
        "OUT_SIZE = 512\n",
        "MARGIN = .2\n",
        "NEGATIVE = ...\n",
        "\n",
        "cnn_net = ...\n",
        "img_net = ImageEncoding(cnn_model=cnn_net, cnn_out_size=CNN_OUT_SIZE, \n",
        "                        out_size=OUT_SIZE) \n",
        "\n",
        "text_net = TextEncoding(text_embedding_size=EMBEDDING_SIZE, out_size=OUT_SIZE)\n",
        "\n",
        "optimizer = optim.Adam([{'params': ...},  # lista de parametros de img_net\n",
        "                        {'params': ...}],  # lista de parametros de text_net\n",
        "                       lr=LR)\n",
        "criterion = TripletLoss(margin=MARGIN, negative=NEGATIVE)\n",
        "scheduler = ... # (opcional) optim.lr_scheduler proporciona varios métodos \n",
        "                # para ajustar el lr según el número de épocas\n",
        "\n",
        "train_triplets_loader = DataLoader(train_flickr_tripletset, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=True, num_workers=2)\n",
        "val_triplets_loader = DataLoader(val_flickr_tripletset, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, meanrr, r10 = train_for_retrieval(img_net, text_net, \n",
        "                                              train_triplets_loader, \n",
        "                                              val_triplets_loader, optimizer, \n",
        "                                              criterion, scheduler, EPOCHS, \n",
        "                                              REPORTS_EVERY, norm=False)\n",
        "\n",
        "plot_results(train_loss, meanrr, 'MRR', r10, 'R@10')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CYP1n6c6_c8"
      },
      "source": [
        "# Test\n",
        "from PIL import Image\n",
        "n_samples = 64\n",
        "\n",
        "# Tomemos n_samples ejemplos del conjunto de test\n",
        "samples = torch.stack([test_flickr_tripletset[i][0] for i in range(n_samples)]).cuda()\n",
        "refs = torch.stack([torch.from_numpy(test_flickr_tripletset[i][1]) for i in range(n_samples)]).cuda()\n",
        "test_caps = [caps[0] for _, caps in test_flickr_set][:n_samples]\n",
        "\n",
        "# Computamos las representaciones en el espacio compartido\n",
        "samples_enc = img_net(samples)['logits']\n",
        "refs_enc = text_net(refs)['logits']\n",
        "\n",
        "# Calculemos las distancias a cada uno de los textos de test y rankeamos\n",
        "dists = torch.cdist(samples_enc.unsqueeze(0), refs_enc.unsqueeze(0), p=2).squeeze(0)\n",
        "ranks = torch.argsort(dists, dim=1)[:,:10]\n",
        "r10 = len([i for i in range(len(ranks)) if len(torch.where(ranks[i,:] == i)[0])]) / len(ranks)\n",
        "\n",
        "# Veamos como se comporta el modelo\n",
        "print(\"Correct Test!\" if r10 >= .25 else \"Failed Test! [R@10]\")\n",
        "\n",
        "# Mostremos las 10 descripciones más cercanas\n",
        "fig, axs = plt.subplots(nrows=n_samples, figsize=(2,n_samples*5))\n",
        "for i in range(n_samples):\n",
        "  axs[i].imshow(Image.open(full_flickr_set.ids[7000+i]))\n",
        "  axs[i].text(600,0,\"EXPECTED:\\n{}: {}\".format(i, test_caps[i]), fontsize=12, fontweight='bold')\n",
        "  axs[i].text(600,750,\"PREDICTED RANK:\\n{}\".format('\\n'.join([f'{j}: {test_caps[j]}' for j in ranks[i]])), fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTd9gUB5xwta"
      },
      "source": [
        "## 2d) Opcional: COCO Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf0hHCkKx0Kz"
      },
      "source": [
        "##############################################################################\n",
        "# Toda esta parte es similar a la anterior pero para COCO Captions.\n",
        "##############################################################################\n",
        "\n",
        "folder_path = './data/coco-caps'\n",
        "if not os.path.exists(f'{folder_path}/images/train2014'):\n",
        "  print('\\n*** Descargando y extrayendo COCO Captions, siéntese y relájese unos 20 mins...')\n",
        "  print('****** Descargando training set...\\n')\n",
        "  !wget http://images.cocodataset.org/zips/train2014.zip -P $folder_path/images\n",
        "  print('\\n********* Extrayendo training set...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
        "  !unzip -q $folder_path/images/train2014.zip -d $folder_path/images && rm $folder_path/images/train2014.zip\n",
        "  print('\\n*** Descargando y extrayendo validation set...\\n')\n",
        "  !wget http://images.cocodataset.org/zips/val2014.zip -P $folder_path/images && unzip -q $folder_path/images/val2014.zip -d $folder_path/images && rm $folder_path/images/val2014.zip\n",
        "  # !wget http://images.cocodataset.org/zips/test2014.zip -P $folder_path/images && unzip -q $folder_path/images/test2014.zip -d $folder_path/images && rm $folder_path/images/test2014.zip\n",
        "  print('\\n*** Descargando y anotaciones de la imágenes...\\n')\n",
        "  !wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P $folder_path && unzip -q $folder_path/annotations_trainval2014.zip -d $folder_path && rm $folder_path/images/annotations_trainval2014.zip\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Resize((32, 32)),\n",
        "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/train2014',\n",
        "                                                   annFile = f'{folder_path}/annotations/captions_train2014.json',\n",
        "                                                   transform=transform)\n",
        "\n",
        "val_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/val2014',\n",
        "                                                 annFile = f'{folder_path}/annotations/captions_val2014.json',\n",
        "                                                 transform=transform)\n",
        "\n",
        "# test_coco_set = torchvision.datasets.CocoCaptions(root=f'{folder_path}/images/test2014',\n",
        "#                                                   transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUZmJrvfJOxs"
      },
      "source": [
        "if not os.path.exists(f'{folder_path}/cap_encodings_512d.pkl'):\n",
        "  !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/cap_encodings_512d.pkl -P $folder_path\n",
        "\n",
        "with open(f'{folder_path}/cap_encodings_512d.pkl', 'rb') as f:\n",
        "  train_cap_encs, val_cap_encs = pickle.load(f)\n",
        "\n",
        "train_coco_tripletset = ImageCaptionDataset(train_coco_set, train_cap_encs)\n",
        "val_coco_tripletset = ImageCaptionDataset(val_coco_set, val_cap_encs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXxbiC0bJY6M"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "LR = 1e-4\n",
        "EPOCHS = 10\n",
        "REPORTS_EVERY = 1\n",
        "CNN_PREV_SIZE = 1024\n",
        "EMBEDDING_SIZE = 512\n",
        "OUT_SIZE = 512\n",
        "MARGIN = .2\n",
        "\n",
        "cnn_net = ...\n",
        "img_net = ImageEncoding(cnn_model=..., cnn_out_size=CNN_PREV_SIZE, \n",
        "                        out_size=OUT_SIZE) \n",
        "\n",
        "text_net = TextEncoding(text_embedding_size=EMBEDDING_SIZE, out_size=OUT_SIZE)\n",
        "\n",
        "optimizer = optim.Adam([{'params': ...},\n",
        "                        {'params': ...}], \n",
        "                       lr=LR)\n",
        "criterion = TripletLoss(margin=...)\n",
        "\n",
        "train_triplets_loader = DataLoader(train_coco_tripletset, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=True, num_workers=2)\n",
        "val_triplets_loader = DataLoader(val_coco_tripletset, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=False, num_workers=2)\n",
        "\n",
        "train_loss, meanrr, r10 = train_for_retrieval(img_net, text_net, \n",
        "                                                     train_triplets_loader, \n",
        "                                                     val_triplets_loader, \n",
        "                                                     optimizer, criterion, \n",
        "                                                     EPOCHS, REPORTS_EVERY, \n",
        "                                                     norm=False)\n",
        "\n",
        "plot_results(train_loss, meanrr, 'MRR', r10, 'R@10')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}